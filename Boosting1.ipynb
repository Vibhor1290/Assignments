{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22a098-fece-46e3-b8c8-e0e021ae74c8",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b623da01-7d15-478f-a15b-1843d9fdbd73",
   "metadata": {},
   "source": [
    "Boosting is a powerful ensemble learning technique used to improve the accuracy of models, especially in classification and regression tasks. The main aim behind boosting is to combine multiple weak learners to create a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8416ea3-f8b9-44cc-a0d8-c649fae3b8a6",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6d30466-f3fe-4881-b8a9-73fa6a95f875",
   "metadata": {},
   "source": [
    "Advantages: \n",
    "    High Accuracy\n",
    "    \n",
    "    Resistance to Overfitting\n",
    "    \n",
    "    Handles Imbalanced Data perfectly\n",
    "    \n",
    "Disadvantages:\n",
    "    \n",
    "    Sensitive to Noisy Data\n",
    "    \n",
    "    Computationally Intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53ce43-1b5b-4e7c-b4c9-b145f9899770",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "raw",
   "id": "402b810e-a39c-4353-ae9c-e571015caaf6",
   "metadata": {},
   "source": [
    "Step1: We get a dataset that we split into train and test.\n",
    "\n",
    "Step2: Once we have this data, we take a sample dataset and fed it to the first weak learner. Before feeding the sample data, we ensure random row and feature sampling is done.\n",
    "\n",
    "Step3: First weak learner give some accuracy along with error. This error causing data is passed on to the next weak learner. Then, this second weak learner again executes with some accuracy and error.\n",
    "\n",
    "Step4: The error data from the second weak learner is fed into the third weak learner. This process keeps on iterating until we reac hthe last learner of the model chain. This last model is called strong learner.\n",
    "\n",
    "Step5: Once this process completes, we get different accuracies from each model. Now to calculate the overall accuracy, we combine the accuracy of all the models but at the same time, we allot specific weightage to each model. This weight depends upon the individual model performance. More the performance, higher will be the weightage and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d6c34-a813-46a0-97cb-574ceb1c141a",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1676f05-80cf-4a66-8bf4-5b5865682bb8",
   "metadata": {},
   "source": [
    "There are 3 types of boosting algorithms:\n",
    "    \n",
    "    1. Gradient Boosting\n",
    "    \n",
    "    2. AdaBoost\n",
    "    \n",
    "    3. Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be9e3a-8e01-4298-bb97-82b8143ed8c5",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0d4ce52-18e0-4dac-a145-647043cf525a",
   "metadata": {},
   "source": [
    "Some of the common paramter in Boosting parameters are mentioned below:\n",
    "\n",
    "1. Number of Estimators: This parameter decides the number of weak learners to be used in the boosting process. An increas in number of estimators can improve model performance but at the same time, it might increase computational complexity and training time.\n",
    "\n",
    "2. Learning Rate: The learning rate controls how much contribution each weak learner has given to get to the final ensemble. A smaller learning rate requires more estimators to achieve the same level of accuracy but can lead to better generalization.\n",
    "\n",
    "3. Regularization Parameters: Boosting algorithms often include regularization parameters(like L1,L2 and gamma) to control model complexity and prevent overfitting.\n",
    "\n",
    "4. Early Stopping: This parameter allows us to cease the boosting process based on certain metric. Early stopping helps prevent overfitting and can save computational resources by stopping the training immediately whenever we observe that model is unable to increase the performance anyfurther."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331d49c-5c77-4fcc-ac1d-f0b3e75d5db4",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7237d0cd-df70-47b2-a21e-29b33ad6c473",
   "metadata": {},
   "source": [
    "Step1: Boosting starts with training of base/weak learner on the entire dataset. This model could be a simple decision tree which makes predictions slightly better than random guessing.\n",
    "\n",
    "Step2: After this first model is trained, the algorithm assigns weights to each data point. In the starting, all data points have equal weights. The model then trains on the data, emphasizing the misclassified points more heavily in subsequent iterations.\n",
    "\n",
    "Step3: The algorithm repeats the training process iteratively, each time adjusting the weights of misclassified data points. The subsequent weak learners focus more on the previously misclassified data, improving the overall performance.\n",
    "\n",
    "Step4: In each iteration, the weak learner predictions are combined with those of the previous models. The combined predictions are weighted based on the models performance, giving more weight to the more accurate models.\n",
    "\n",
    "Step5: After achieving either a predefined iteration or performance threshold, the boosting combines the predictions of all weak learners to make the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c43672-87f1-4bac-b677-9e3d6516cee3",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "raw",
   "id": "05f65399-f86f-459c-8db6-bb3193ef7a63",
   "metadata": {},
   "source": [
    "Adaptive Boosting, is a popular ensemble learning algorithm used for binary classification tasks. Just like simple boosting, this model also combines multiple weak learners to create a strong learner."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fbf9bf0-2a30-49e4-b54f-92256717f064",
   "metadata": {},
   "source": [
    "Step1: We initially assign equal weights to all training samples and then choose a weak learner to start our boosting.\n",
    "\n",
    "Step2: Then, we train the weak learner on the training data with the initialized weights. After that, we evaluate the error of weak learner by adding the weights of misclassified samples. At this stage only, we keep on updating the weights.\n",
    "\n",
    "Step3: We keep on repeating this process by choosing another weak learner, adjusting its weights, and updating sample weights.\n",
    "\n",
    "Step4: Later, we combine the weak learner's predictions using a weighted sum or a weighted majority vote.\n",
    "\n",
    "Step5: Finally, we use the combined model to make predictions on new data. The final prediction is typically based on a weighted vote or sum of the weak learners' predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfc96d-8ffc-42ea-8439-460ba4a7a2db",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fc8a07d-a204-4331-9011-0ef51b483f57",
   "metadata": {},
   "source": [
    "AdaBoost uses an exponential loss function to measure the performance of weak learners during the training process.\n",
    "\n",
    "FORMULA:\n",
    "\n",
    "L(y, f(x)) = e**-(y.f(x))\n",
    "\n",
    "where: y is the true label of sample,\n",
    "       f(x) is prediction made by weak learner for sample x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b43d0b1d-2e56-4be2-841c-ca05800e7737",
   "metadata": {},
   "source": [
    "The exponential loss function penalizes misclassifications exponentially. When the weak learner makes a correct prediction, the loss reaches very close to 0. On the other hand, when the prediction is incorrect, the loss increases rapidly.\n",
    "\n",
    "As a result, AdaBoost can effectively learn from mistakes and build a strong ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003c5a0-a036-49e8-9c91-9abc63bfb1c6",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1270e8b0-fa8e-4fe9-bc85-f872c2d5b553",
   "metadata": {},
   "source": [
    "Step1: Initially, all samples in the training set have equal weights.\n",
    "\n",
    "Step2: At each iteration, a weak learner is trained on the training data with the current weights.\n",
    "\n",
    "Step3: The weak learner will makes certain predictions for each sample. The weighted error of the weak learner is calculated as the sum of weights of misclassified samples.\n",
    "\n",
    "Step4: The weight of the weak learner in the final model is computed based on its weighted error.\n",
    "\n",
    "Step5: The weights of misclassified samples are updated in this step. Also, correctly classified samples have reduced weights, while misclassified samples have increased weights thus making them more influential in the next iteration.\n",
    "\n",
    "Step6: After weights updating is done, they are normalized so that they all sum equal to 1.\n",
    "\n",
    "By updating the weights of misclassified samples, AdaBoost focuses more on difficult-to-classify samples in each iteration, leading to a strong ensemble model that performs well even on complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba5219-905a-4099-882c-c667fed4d7e0",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7289c994-6af6-498e-ad55-549858ce59b6",
   "metadata": {},
   "source": [
    "1. If we increase the number of estimators, it will result in improved accuracy especially when we are dealing with complex datasets. By doing so, the model will have more opportunity to explore different sample dataset and reduce the chances of biasness.\n",
    "\n",
    "2. It is not always fruitful to increase the number of estimators. Due to higher number of estimators, we might encounter risk of overfitting, especially when the weak learners are complex or if the dataset is having noise.\n",
    "\n",
    "3. As obvious, training time will increase with the number of estimators since each iteration involves training a new weak learner and updating sample weights.\n",
    "\n",
    "4. There is a situation when we face diminishing returns. Under this case, we start losing the model performance and accuracy when we keep on adding more and more features. However, adding more features leads to increase in performance but there is a certain limit up to which we can add the features without hampering the model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec700dd-d43d-4fe4-9d4c-c0142b9e4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
