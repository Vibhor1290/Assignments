{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27e137a-419a-4475-bc2b-7eea12a65de5",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03abdf74-9a0f-4741-a777-f8aeaed77204",
   "metadata": {},
   "source": [
    "Lasso Regression is a type of linear regression that implements L1 regularization. The main motive behind Lasso Regression is to reduce the coefficients towards zero with the help of which we achieve both feature selection and multicollinearity reduction and thus, improves the model interpretability and prediction performance.\n",
    "\n",
    "Lasso Regression tends to be more interpretable than OLS and Ridge because it produce sparse models thus making it easier to understand which features are impacting the dependent variable the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14575701-ad3f-4fa8-9dab-23792170ec68",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2d515c3-415b-4d20-bed0-a5c81db0b664",
   "metadata": {},
   "source": [
    "The main advantages of using Lasso regression is as follows:\n",
    "\n",
    "1. Lasso Regression automatically removes irrelevant or less important features by reducing their coefficients to zero.\n",
    "\n",
    "2. With lesser number of features, model will become easier to interpret and will be clear to distinguish what all features are impactful.\n",
    "\n",
    "3. By excluding irrelevant features, Lasso reduces the risk of overfitting. Overall, a simpler model with fewer features is comparatively less expensive and faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab1e5b-ecc3-4c8c-aefb-edfa7c2f36de",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d920b93-5137-4a96-97a6-75dc814d88dd",
   "metadata": {},
   "source": [
    "In Lasso Regression model, many coefficients might be exactly zero. The non-zero coefficients represents those features that are consideraby important for the model while the zero coefficients represents those features which the model has simply denied.\n",
    "\n",
    "The magnitude of the non-zero coefficients shows how much relationship is there between independent and dependent feature. A larger value shows a strong relation while a small value shows weak relation.\n",
    "\n",
    "The sign of the coefficient indicates either direct or inverse relationship. A positive coefficient means that with increase in independent feature, the dependent feature will also increase.\n",
    "On the other hand, a negative coefficient means that with increase in independent feature, the dependent feature will decrease and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e850b9-94e9-485c-a3f5-6e7447e8e516",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "611c4a13-c950-4d6f-9408-03b390de85f7",
   "metadata": {},
   "source": [
    "There are certain tuning parameter that we can use in Lasso Regression:\n",
    "    \n",
    "    1. Regularization paramter (lambda):\n",
    "    \n",
    "    If lambda = 0, there will be no reduction in the coefficients but it might increase the chances of overfitting.\n",
    "    \n",
    "    If lambda is small, coefficients are reduced by only a slight margin which leads to some reduction in overfitting.\n",
    "    \n",
    "    If lambda is moderate, some of the coefficients are set to zero, resulting in feature selection and model becomes simpler and less overfitting.\n",
    "    \n",
    "    If lambda is high, majority of the coefficients are brought down to zero creating a very sparse model. The model becomes simple however, leads to underfitting of data.\n",
    "    \n",
    "    2. Maximum Number of Iterations (max_iter):\n",
    "    \n",
    "    A higher iteration value ensures that the algorithm has sufficient opportunity to converge, especially for larger datasets or complex models.\n",
    "    However, it increases computational time.\n",
    "    \n",
    "    3. Normalization:\n",
    "    \n",
    "    This parameter can also be very important whenever the independent features are on different scales. It ensures that regularization term penalizes all coefficients equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec261ff-cbb9-40a3-a969-e1bfce7a3178",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "492de9cf-8e8a-4efa-97d1-8fb3bef6c852",
   "metadata": {},
   "source": [
    "Lasso Regression is basically a linear model which builds a linear relationship between input and output features. However, it can be modified a little bit to handle non-linear relationships via few approaches. Some of the approaches are mentioned below:\n",
    "\n",
    "1. Transformation of original features into polynomial features using polynomial terms (squared, cubed, etc.) and interaction terms (products of pairs of features). For transformation part, we can use Pipeline concept.\n",
    "\n",
    "2. Using RBF(Radial Basis Function) depend on the distance from a center point. They can capture complex, non-linear relationships.\n",
    "\n",
    "3. Although Lasso is linear but it can be used in combination with non-linear models. Example: ensemble methods like stacking can combine linear and non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9c4c5-e36f-47a0-96ad-da5172f65103",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f5875-5ec7-4be8-b472-5baa9294e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main difference between Ridge and Lasso regression is that "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d07c67a-593b-4efd-9713-161fbb232283",
   "metadata": {},
   "source": [
    "Ridge Regression:\n",
    "\n",
    "1. Redcues the coefficients of features towards zero(but not exactly to zero). By doing this, Ridge Regression retains all the features in the model but with smaller weights.\n",
    "\n",
    "2. Suitable for cases where all features are somewhat relevant but multicollinearity is a concern.\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "1. Reduces the coefficients of features exactly to zero hence achieving feature selection. This results in a sparse model where some features are completely ignored thus creating  a simpler and easily interpretable model.\n",
    "\n",
    "2. Suitable for cases where only a subset of features is believed to be relevant and the rest can be excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d279e-f871-4470-86b8-ec8ce906402d",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61581dee-8af6-4739-afb0-f07cddf708a7",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity present within input features. Multicollinearity occurs when two or more independent features are highly correlated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fd6d1dc-18c5-4e06-b284-2e26df10352b",
   "metadata": {},
   "source": [
    "L1 regularization:\n",
    "\n",
    "Lasso Regression implements L1 penalty to the coefficient's value. This penalty term reduces some of the coefficients to zero hence completely ignoring them from the model.\n",
    "\n",
    "When multicollinearity is there, then Lasso selects one feature out of the group of highly correlated features and reduce the rest to zero. This will create a simple model that retains only the informative feature.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "This is somewhat similar to L1 regularization. By setting some coefficients to zero, Lasso performs implicit feature selection which helps in identifying and retaining only the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18391306-23e0-457d-a3c3-6ce953a65971",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee8afe4b-28d3-4d91-a670-8370997e5542",
   "metadata": {},
   "source": [
    "We can perform various hyper-parametertuning techniques like:\n",
    "    \n",
    "    1. K-Fold Cross Validation\n",
    "    \n",
    "    2. Randomised Search\n",
    "    \n",
    "    3. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0422482-10cd-4cff-80e9-a67fe85f2e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
