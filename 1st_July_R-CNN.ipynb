{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61622c70-e6cf-4852-a0ba-f496c74ab0fb",
   "metadata": {},
   "source": [
    "1. What are the objectives of using Selective Search in R-CNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "131d01da-6087-4a5f-90f9-a82bf4a8aa97",
   "metadata": {},
   "source": [
    "The main objectives of using Selective Search in R-CNN are mentioned below:\n",
    "\n",
    "1. Selective Search generates a diverse set of region proposals that are likely to contain objects. This helps in reducing computational load by focusing only on potentially relevant regions rather than processing of entire image.\n",
    "\n",
    "2. By providing a set of region proposals, Selective Search helps R-CNN in accurately localizing objects within these proposals.\n",
    "\n",
    "3. Selective Search focuses on improving the accuracy of object detection models by proposing regions that cover objects of varying sizes, shapes, and textures thus capturing a wide range of object instances present in the image.\n",
    "\n",
    "4. Instead of applying object detection densely over all possible image regions, Selective Search reduces the number of regions to be processed which reduces the computational costs and speeding up of inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ff4eb-6182-45b8-a650-f3c3d66c7d37",
   "metadata": {},
   "source": [
    "2. Explain the following phases involved in R-CNN:\n",
    "\n",
    "Region proposal\n",
    "\n",
    "Warping and Resizing\n",
    "\n",
    "Pre trained CNN architecture\n",
    "\n",
    "Pre Trained SVM models\n",
    "\n",
    "Clean up\n",
    "\n",
    "Implementation of bounding box"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8b77e0e-daca-4e22-8399-f1c40aaf1cbd",
   "metadata": {},
   "source": [
    "1. Region Proposal is the first phase of R-CNN in which potential regions are generated in an image that are likely to contain objects. This is usually done using Selective Search.\n",
    "\n",
    "2. Once the region proposals are generated, they are warped and resized to become compatible with the input size requirements of the underlying Convolutional Neural Network (CNN).\n",
    "\n",
    "3. R-CNN utilizes a pre-trained Convolutional Neural Network as a feature extractor which has been trained on a large dataset to learn discriminative features from images.\n",
    "\n",
    "4. Once feature extraction is performed by CNN, a Support Vector Machine classifier is trained to classify each region proposal into different object classes (like person, animal, car, etc)\n",
    "\n",
    "5. After classification, we use merging or filtering in any redundant or overlapping region proposals such that each detected object instance is represented by a single, best-fitted bounding box. We generally use NMS(Non Max Suppression) technique in this stage.\n",
    "\n",
    "6. Atlast, the refined bounding boxes around detected objects are implemented on the original image to visually depict the locations and extents of the recognized objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892cde0-9a7c-4e9d-80e6-a5fa65e7d1cc",
   "metadata": {},
   "source": [
    "3. What are the possible pre-trained CNNs we can use in Pre-trained CNN architecture?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a197d517-f1fa-4ebc-b92d-04babbc89244",
   "metadata": {},
   "source": [
    "There are several pre-trained CNN architectures commonly used in various object detection and image recognition tasks. Some of them are listed below:\n",
    "\n",
    "1. VGG(Visual Geometry Group) models like VGG16 and VGG19 are known for their simplicity and effectiveness. They have a uniform architecture with a stack of convolutional layers followed by fully connected layers.\n",
    "\n",
    "2. ResNet models like ResNet50, ResNet101, ResNet152 introduced the concept of residual connections(skip connections) that helped us to train deeper networks more effectively. Due to this, they become a standard choice for many computer vision tasks.\n",
    "\n",
    "3. Inception models like InceptionV3 and InceptionResNetV2 uses inception modules that facilitated efficient use of computation and parameters by using different kernel sizes within the same layer.\n",
    "\n",
    "4. EfficientNet models such as EfficientNetB0 to EfficientNetB7 scaled up in both depth and width to achieve better performance resulting in a good balance model between size and computational cost effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75c2d6-e488-4201-bfeb-c32096ecafb7",
   "metadata": {},
   "source": [
    "4. How is SVM implemented in the R-CNN framework?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbf6bed8-90d7-425a-ae9a-c42c8e1e41d6",
   "metadata": {},
   "source": [
    "1. Initially, regions of interest (RoI) are generated using a selective search algorithm or a similar method. Each RoI is a bounding box that potentially contains an object.\n",
    "\n",
    "2. For each RoI, features are extracted which is typically performed using a pre-trained deep convolutional neural network such as VGG16 or ResNet, which is typically fine-tuned for better feature representation.\n",
    "\n",
    "3. Once the feature extraction is done for each RoI, they are fed into an SVM classifier where SVM is trained to classify each RoI into one of the predefined classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88cf52d-5a84-478e-9574-45953f32a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n"
     ]
    }
   ],
   "source": [
    "# lets look at an example:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, Y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=35)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n",
    "\n",
    "model = svm.SVC(kernel='linear')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2ff9e-f85b-4514-a5f8-883a6f9c456c",
   "metadata": {},
   "source": [
    "5. How does Non-maximum Suppression work?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a96e39e8-16f4-42ad-9dac-4b855c9cc379",
   "metadata": {},
   "source": [
    "NMS is a technique used in object detection and computer vision tasks to filter out any redundant detection of objects that overlap significantly. It make sure that for each object instance detected, only the most confident and representative detection is kept.\n",
    "\n",
    "Step1: At first, we get an output of bounding boxes from object detection algorithm. These bounding boxes represent potential detections of objects in an image.\n",
    "\n",
    "Step2: Then, we sort the bounding boxes on the basis of confidence scores in descending order. The confidence score shows how likely it is that the bounding box contains an object of interest.\n",
    "\n",
    "Step3: Then, we start with the bounding box having highest confidence score and mark it as a detection. Later, we calculate the Intersection over Union overlap between this highest confidence box and all other remaining boxes.\n",
    "\n",
    "Step4: We remove any bounding boxes that has a high IoU with the selected box and keep only those boxes that have a low IoU with the selected box. Here, we set a threshold value on the basis of which we remove the bounding boxes.\n",
    "\n",
    "Step5: We keep on repeating these steps until we achive a minimal number of bounding boxes. The final output is a set of bounding boxes with their associated confidence scores, where each bounding box represents a unique detection of an object after non-maximum suppression has been applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432a27f-ed4c-4223-8c11-9c0eba400a8f",
   "metadata": {},
   "source": [
    "6. How Fast R-CNN is better than R-CNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ec40ca9-e6b7-4de9-8a2a-fb888eb1a662",
   "metadata": {},
   "source": [
    "1. In Fast R-CNN, the entire object detection pipeline is integrated into a single deep convolutional network whereas in R-CNN, object proposals are generated separately and processed individually.\n",
    "\n",
    "2. Fast R-CNN implemented RoI pooling that allowed more efficient extraction of features from the region proposals generated using selective search. Further, RoI pooling enabled sharing of computation across overlapping regions, making it faster as compared to separate processing of each proposal in R-CNN.\n",
    "\n",
    "3. While Fast R-CNN achieves faster inference times, it also helps in achieving better accuracy as compared to R-CNN. This is attributed for better feature extraction through RoI pooling and the benefits of end-to-end training, which increases the overall discriminative power of the model.\n",
    "\n",
    "4. Unlike R-CNN, Fast R-CNN simplifies the object detection pipeline by removing the need for multiple stages of processing and integrating the entire process into a more unified and efficient framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8544b-a804-4568-bc98-b885f3f3de17",
   "metadata": {},
   "source": [
    "7. Using mathematical intuition, explain ROI pooling in Fast R-CNN."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5e0bb03-b358-4286-abfb-6df4d810b5f9",
   "metadata": {},
   "source": [
    "The main purpose of ROI pooling is: \n",
    "    \n",
    "    Feature Extraction\n",
    "    \n",
    "    Spatial Alignment\n",
    "    \n",
    "MATHEMATICAL INTUITION:\n",
    "\n",
    "1. Lets say we have a region proposal A specified by its bounding box coordinates.\n",
    "\n",
    "2. Now, we divide this A into nxn spatial bins where each bin corresponds to a region in the feature map.\n",
    "\n",
    "3. For each bin, we apply a pooling operation (like max pooling) to extract a single value.\n",
    "\n",
    "4. Then, we Concatenate all the pooled values into a fixed-size output vector representing the region proposal A."
   ]
  },
  {
   "cell_type": "raw",
   "id": "acebf975-ee4d-4e5b-a860-5556cb32e78e",
   "metadata": {},
   "source": [
    "BENEFITS:\n",
    "    \n",
    "     RoI pooling ensures that extracted features are invariant to small spatial translations or deformations within the region proposal thus making the model more robust to variations in object position and scale.\n",
    "     \n",
    "     RoI pooling reduces the region proposal dimensionality while retaining important spatial information, further enhancing the computational efficiency and reduce memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146b1d1-f4ac-4d18-8dc2-0ddb6eb7f678",
   "metadata": {},
   "source": [
    "8. Explain the following processes:\n",
    "      \n",
    "      a. ROI Projection\n",
    "      \n",
    "      b. ROI pooling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d1b75df-a9e2-48f1-85c6-36aa42e29c1d",
   "metadata": {},
   "source": [
    "ROI Projection refers to the mapping of a region proposal (ROI) from the original image space onto the feature map generated by a convolutional neural network. ROI projection works as follows:\n",
    "\n",
    "1. At first, an input image is fed into convolutional operations in a CNN which creates a feature map. This feature map captures hierarchical representations of the image, encoding spatial features at different scales.\n",
    "\n",
    "2. A region proposal algorithm identifies the potential object locations within the image, generating bounding boxes that are likely to contain objects.\n",
    "\n",
    "3. Each ROI in the original image is then mapped onto the corresponding region in the feature map. This is done to ensure that features extracted from CNN correspond precisely to the spatial locations specified by the ROI.\n",
    "\n",
    "4. Finally, to project an ROI onto feature map, we use spatial scaling and alignment. After projection, each ROI is associated with a corresponding region in the feature map, facilitating subsequent operations such as RoI pooling for feature extraction within each ROI."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5cd65bf-f39b-45a0-a03a-5b0b5ab17732",
   "metadata": {},
   "source": [
    "On the other hand, ROI Pooling is a technique used after performing ROI projection to extract a fixed-size feature representation from each region proposal within the feature map.\n",
    "\n",
    "1. After performing ROI projection, each ROI is represented within the feature map as a region defined by its spatial coordinates.\n",
    "\n",
    "2. Then, we divide each ROI region into a fixed grid of smaller spatial bin. This process divides the ROI into a set of regions that can be individually processed.\n",
    "\n",
    "3. Later, we apply pooling(Max Pooling) operation within each spatial bin of the grid which extracts a single value from each bin.\n",
    "\n",
    "4. After pooling operation, we concatenate all the pooled values into a fixed-size output vector and this vector represents a fixed-size feature representation of the ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b0e50-9bfc-4a85-8904-cfe3f4662b49",
   "metadata": {},
   "source": [
    "9. In comparison with R-CNN, why did the object classifier activation function change in Fast R-CNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c463283f-4bad-4309-8e92-d454f630a18f",
   "metadata": {},
   "source": [
    "The object classifier activation function changed in Fast R-CNN because of following reasons:\n",
    "\n",
    "1. Unlike R-CNN, which involved separate stages for region proposal and classification, Fast R-CNN integrated these stages into a single network. This integration allows for end-to-end training and efficient processing of region proposals.\n",
    "\n",
    "2. Using softmax activation function in Fast R-CNN was found to be computationally expensive, especially when we have large number of region proposals and classes.\n",
    "\n",
    "3. Fast R-CNN used logistic regression(sigmoid activation function) for binary classification tasks. This function simplified the classification process for each region proposal to a binary decision, which proved to be computationally efficient and straightforward.\n",
    "\n",
    "4. For multi-class classification tasks, Fast R-CNN used linear activation function followed by softmax layer in the final classification stage after RoI pooling. This approach collects information from multiple proposals efficiently without redundantly applying softmax to each proposal individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e232e-b270-43e2-b824-8ef4996407e3",
   "metadata": {},
   "source": [
    "11. Explain the concept of Anchor box."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f683f0e9-64c3-4375-ae32-e972cb611179",
   "metadata": {},
   "source": [
    "Anchor box is a widely used concept in object detection algorithms, especially in convolutional neural network based approaches like Faster R-CNN, YOLO, and SSD. They play a vital role in handling object variability in terms of scale, aspect ratio, and position within an image.\n",
    "\n",
    "Objects present in a image might vary in terms of size, aspect ratio, and position. Thus, anchor boxes provide a mechanism to efficiently handle this variability by pre-defining a set of boxes with different sizes and aspect ratios. Anchor boxes are used for both object localization and object classification.\n",
    "\n",
    "IMPLEMENTATION:\n",
    "\n",
    "1. Prior to model training, anchor boxes are generated by placing them at predefined locations over the image grid where these locations are determined by the stride of convolutional layers in the CNN.\n",
    "\n",
    "2. During training phase, anchor boxes are matched with ground truth objects. This matching determines which anchor box to be considered as either positive and negative.\n",
    "\n",
    "3. The predictions made by the model are compared against the ground truth values. This comparison forms the basis of the loss function used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd4af4-a767-4027-8f2a-26148491e685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
