{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab86a31b-2329-4e82-a7b2-28a69f9efe1a",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cbaf440-8420-45ca-a90f-200e47a95498",
   "metadata": {},
   "source": [
    "The eigenvectors simply represents the directions of principal components and eigenvalues represent the magnitude of the variance that each principal component holds."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8711e1c-0ff0-4eef-8b17-ed71d58e664e",
   "metadata": {},
   "source": [
    "Eigen-Decomposition is an approach used to split or break down a magtix into eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61af7ee5-f4c6-417b-bad0-01e52fd07d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd419a13-da0e-4097-9cdd-3aaaaa0e02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we have a 2x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39ab239-98d0-431c-ba8c-26b19671d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[4,1],[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa909978-c593-4aa3-874a-293f502ef809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[4, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6de5be6-9354-405d-a0e5-116f2715bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split this matrix into eigenvector and eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bdaa12-506e-4777-bf39-3242e9fadaf7",
   "metadata": {},
   "source": [
    "# For eigen values:\n",
    "\n",
    "A - lambda*I = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43f8d7f0-e38b-4a0a-ab57-1e898e80b2a8",
   "metadata": {},
   "source": [
    "Here, A is our main matrix and I is an identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1a1ca16-9a2f-43ec-b029-d32b5a92172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "I  = np.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f82d9ccf-2289-4f12-aee5-24a064401aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ea136-56b0-4a85-998e-31f9632f93f8",
   "metadata": {},
   "source": [
    "We will apply A - lambda*I = 0\n",
    "\n",
    "From here, we will get two values of lambda, 5 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b80d17-8bde-4ecb-aee6-a9f1bd8abbf4",
   "metadata": {},
   "source": [
    "# For eigen vectors\n",
    "\n",
    "A - lambda(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30bc1f-839d-403c-8e2c-9b676b013a8c",
   "metadata": {},
   "source": [
    "here, we will take lambda as both 5 and 2 separately to get two eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "398e0fb7-2654-415e-8a95-9a2ac9588478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get the below mentioned eigenvectors:\n",
    "\n",
    "EV1 = np.matrix([[1],[1]])\n",
    "EV2 = np.matrix([[1],[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9fc8eba-41f7-41f4-9999-10b2b438f878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1],\n",
       "         [1]]),\n",
       " matrix([[ 1],\n",
       "         [-2]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EV1, EV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18073d9b-102c-4b5e-812a-519b26b5acc6",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e0fae21-c826-49a3-9582-97cd72396e5f",
   "metadata": {},
   "source": [
    "Eigen-decomposition is a way of factorizing a square matrix into a canonical form, where the matrix is expressed in terms of its eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcfb62e7-d418-40e3-9168-313eb7574076",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors provides a deep insights into properties of matrix. For example, the eigenvalues of a matrix indicate whether the matrix is invertible or not, the nature of its determinant, and its trace.\n",
    "\n",
    "Eigen-decomposition is a key component of Principal Component Analysis to reduce the dimensionality of data thus enhancing computational efficiency and improving model performance.\n",
    "\n",
    "Many matrix functions, such as matrix exponentials, powers, and logarithms, can be more easily computed when the matrix is diagonalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32a142-2fd8-455b-aa30-4adb80dfffd5",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d04b35f-b7b3-4869-a58c-d7c0720d5f4f",
   "metadata": {},
   "source": [
    "For a square matrix to be diagonalizable using the Eigen-Decomposition approach, it must satisfy the following conditions:\n",
    "    \n",
    "    1. Matrix must have n number of linearly independent eigenvectors along with their corresponding eigenvalues."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f90bf26-6b8b-4813-bf54-f4c10b3410ac",
   "metadata": {},
   "source": [
    "PROOF: \n",
    "    Lets say a square matrix M. So, there also exists inverse matrix P and diagonal matrix D such that:\n",
    "        \n",
    "        M = P.D.P_inv\n",
    "        \n",
    "        here, the columns of P are eigenvectors of matrix M,\n",
    "              daigonal elements of D are corresponding eigenvalues of M"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0aa97ee6-30ad-4501-990f-ac16590f624d",
   "metadata": {},
   "source": [
    "Since P is invertible which means its columns must be linearly independent. Hence, there are n linearly independent eigenvectors of M."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee3dd8a8-4a58-44dc-b651-cfa3461dad2c",
   "metadata": {},
   "source": [
    "Now consider the eigenvalue λ with algebraic multiplicity m. The characteristic polynomial det(M−λI)=0 has λ as a root m times.\n",
    "    \n",
    "Since M is diagonalizable, the eigenspace corresponding to λ must have m number of linearly independent eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac095df1-b806-4f6d-9f21-d7cd1d5b4555",
   "metadata": {},
   "source": [
    "What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "raw",
   "id": "450e38f0-bd91-49c3-bff9-6bd7e59444e9",
   "metadata": {},
   "source": [
    "The spectral theorem states that:\n",
    "\n",
    "1. A real symmetric matrix A can be diagonalized by an orthogonal matrix P, provided that A = P.D.P[t] where:\n",
    "    \n",
    "    D is a diagonal matrix containing the eigenvalues of A and \n",
    "    \n",
    "    P is an orthogonal matrix whose columns are the normalized eigenvectors of A.\n",
    "    \n",
    "2. A complex Hermitian matrix A can be diagonalized by a unitary matrix U, provided that A=U.D.U[t] where:\n",
    "    \n",
    "    D is a diagonal matrix containing the eigenvalues of A and\n",
    "    \n",
    "    U is a unitary matrix whose columns are the normalized eigenvectors of A.\n",
    "    \n",
    "\n",
    "Significance in Eigen-Decomposition:\n",
    "\n",
    "1. The eigenvectors can be chosen to be orthonormal, which simplifies computations and improves numerical stability.\n",
    "\n",
    "2. The resulting diagonal matrix contains all the eigenvalues of the original matrix thus making it simpler to understand the matrix behavior, especially in terms of its spectral properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cef3ff-45e8-4cb9-a6e1-73d5fd32e403",
   "metadata": {},
   "source": [
    "EXAMPLE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b0f8a4-e3bd-4887-b151-7c8d52b7fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.matrix([[4,1],[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfcdc395-6ea7-4632-ac2d-fcfd489f259c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[4, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e474b-60de-41e0-a9cb-2a066092f10e",
   "metadata": {},
   "source": [
    "# For calculating eigenvalues:\n",
    "\n",
    "M - λ*(I) = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ca940b9-9c1f-4c31-a5e8-00fe4e0e2c05",
   "metadata": {},
   "source": [
    "Here, M is our main matrix and I is an identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40d5f8-bdd9-435a-9726-23c18470877e",
   "metadata": {},
   "source": [
    "From here, we will find out two eigenvalue, 5 and 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a3458-e13f-4a4e-934a-28a626285f8a",
   "metadata": {},
   "source": [
    "# For calculating eigenvectors:\n",
    "\n",
    "M - λ*(I) = 0\n",
    "\n",
    "we will put both the values 5 and 2 separately to get two different eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2234d6b-1199-4c3f-bebe-6e279d61d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_1 = np.matrix([[1],[1]])\n",
    "EV_2 = np.matrix([[1],[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15f182d4-2d9e-4b2c-9c7a-1a4ba5f4401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[1],\n",
       "         [1]]),\n",
       " matrix([[ 1],\n",
       "         [-2]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EV_1,EV_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bb697-dde1-408f-8ac9-ace26e6be3b7",
   "metadata": {},
   "source": [
    "# Now, we will normalize eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f75d4597-077b-4ffe-82b0-1a1689db410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vec1 = EV_1/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1bbe4af-f3f3-47a9-88ef-512272d13439",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vec2 = EV_2/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45edf136-e8d9-4812-9412-bfc37b4aee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.70710678],\n",
       "        [0.70710678]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7eb21e5-47be-49ac-a46a-6a1a6c0ad7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.70710678],\n",
       "        [-1.41421356]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef10eb16-ee36-4640-ae0a-a896b90baabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also, we have the following matrices:\n",
    " \n",
    "Diagonal = np.matrix([[5,0],[0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fca15010-c06b-4341-9d3b-8025bbbe0d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5, 0],\n",
       "        [0, 2]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2edec177-507b-4a4a-afdb-1db111e0e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orthogonal_Matrix = np.matrix([[1/np.sqrt(2),1/np.sqrt(2)],[1/np.sqrt(2),-1/np.sqrt(2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28515636-b5ce-408d-9022-4a2a9d4537ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.70710678,  0.70710678],\n",
       "        [ 0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Orthogonal_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d8438aa-4f04-48e1-a8d2-ca3829905383",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ortho_trans = Orthogonal_Matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a228a552-0689-4a18-b42f-3d9500935d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.70710678,  0.70710678],\n",
       "        [ 0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ortho_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb7e1d-df8f-423f-ad50-14e7155f65cf",
   "metadata": {},
   "source": [
    "# To verify the diagonlization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312dd95-cc92-4755-b403-65edee6d5fff",
   "metadata": {},
   "source": [
    "M = PDP[t]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78588ca7-5a02-429a-bfc2-c1d18b7780b5",
   "metadata": {},
   "source": [
    "here: P is the orthogonal matrix,\n",
    "      D is the diagonal matrix, and \n",
    "      P[t] is the transpose of orthogonal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e53410-10ec-47a3-8e3c-f06274c0781a",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f92569aa-fa07-4a64-801f-949b38132b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[4, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e1a7108-9dd1-4092-860d-ee27f42203f7",
   "metadata": {},
   "source": [
    "To calculate the eigenvalues of M:\n",
    "    \n",
    "    We use the formula M - λ*(I) = 0\n",
    "    \n",
    "This will form a quadratic equation with the help of which we will find the possible eigenvalues.\n",
    "\n",
    "In this case, we will get the eigenvalues as 5 and 2."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d387d91a-174a-4c44-bf27-b4a01c59cc7e",
   "metadata": {},
   "source": [
    "1. Eigenvalues of a matrix represents a quadratic equation which can give information about principal axes and principal moments of inertia in mechanics.\n",
    "\n",
    "2. In techniques like Principal Component Analysis, eigenvalues of the covariance matrix denotes the variance captured by each principal component, helping in dimensionality reduction and data compression.\n",
    "    \n",
    "3. An eigenvalue also represents a factor by which the corresponding eigenvector is scaled during the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368da20e-a2f5-4d41-b62e-d691fc68c870",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3233a66e-0c64-4268-a80c-d8cebf424ed3",
   "metadata": {},
   "source": [
    "Some of the real world applications of Eigen decomposition are:\n",
    "    \n",
    "    1. Eigenvalues and eigenvectors are used to study the modes of response of linear systems, design controllers, and optimize system performance in applications like robotics, aerospace, and automation.\n",
    "    \n",
    "    2. Eigen decomposition is employed in various image and signal processing tasks like image denoising, compression, and feature extraction. Technology like facial recognition rely on eigen decomposition principles.\n",
    "    \n",
    "    3. Eigen decomposition is used in chemical to analyze the rate constants and reaction mechanisms of chemical reactions.\n",
    "    \n",
    "    4. Eigen decomposition techniques are applied in recommendation systems like we see in e-commerce platforms, streaming services, and social media platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d332dcf-5f72-4930-9364-dceafc8a39a8",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b8c7945-4dc4-44cf-aa46-590ce14298ac",
   "metadata": {},
   "source": [
    "Generally, a square matrix typically has only one set of eigenvalues and corresponding eigenvectors. However, there are certain scenarios and exceptions to this general rule.\n",
    "\n",
    "1. If a matrix has repeated eigenvalues, it is possible that the matrix has multiple linearly independent eigenvectors corresponding to that eigenvalue.\n",
    "\n",
    "2. In some cases, a matrix may be defective due to which it does not have a complete set of linearly independent eigenvectors. This usually occurs when the algebraic multiplicity is greater than the geometric multiplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2cfa5-0e16-4c27-b36f-243d721ddd5e",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "raw",
   "id": "009d3c75-03c5-410e-8812-a8888e97faa4",
   "metadata": {},
   "source": [
    "Below are the 3 specific applications or techniques that rely on eigen-decomposition:\n",
    "\n",
    "1. PCA\n",
    "\n",
    "PCA is a very popular dimensionality reduction technique used in data analysis and machine learning.\n",
    "\n",
    "It relies on eigen-decomposition to find out principal components of a dataset, which are orthogonal vectors and captures the maximum variance.\n",
    "\n",
    "By showing the data in terms of principal components, PCA can reduce the dimensionality of the dataset while retaining most of its variance thus making it easier to understand and analyze high-dimensional data.\n",
    "\n",
    "2. Eigenfaces is a facial recognition\n",
    "\n",
    "This technique uses eigen-decomposition to represent faces in terms of linear combination of eigenfaces, which are eigenvectors of a covariance matrix computed from a set of face images.\n",
    "\n",
    "By decomposing the face images into eigenfaces, the dimensionality of the face is reduced and thus facial features are represented in a more compact and discriminative manner.\n",
    "It compares the similarity between the eigenface representations of different faces, making it a popular approach in biometrics and security systems.\n",
    "\n",
    "3. Singular Value Decomposition\n",
    "\n",
    "SVD is a matrix factorization technique that relies on eigen-decomposition for its evaluation. It decomposes a matrix into three constituent matrices, where 2 of them are orthogonal matrices of eigenvectors, and one is diagonal matrix of singular values.\n",
    "\n",
    "SVD is used in applications like image compression, collaborative filtering in recommendation systems, and solving linear least squares problems. In ML, SVD is also employed in techniques like latent semantic analysis (LSA) for text analysis and collaborative filtering in recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4d258-ad19-4802-b73e-1e973daeb236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
