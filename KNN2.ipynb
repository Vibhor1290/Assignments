{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb3cda0-a133-4f13-a79b-e4e3d8b2600d",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77c86444-5f35-489f-bd5f-723c0e36e64a",
   "metadata": {},
   "source": [
    "Euclidean distance denotes the shortest straight line distance between two points in Euclidean space.\n",
    "\n",
    "While, Manhattan distance represents sum of the absolute differences along each dimension\n",
    "\n",
    "IMPACT ON KNN PERFORMANCE:\n",
    "\n",
    "1. Euclidean Distance might get heavily impacted by outliers since squared differences intensify their impact. On the other hand, Manhattan distance is less affected by outliers due to the use of absolute differences.\n",
    "\n",
    "2. Euclidean Distance treats all dimensions equally, which can be very helpful if all dimensions contribute equally to similarity. Also, Manhattan Distance focuses more on directional changes along each dimension which helps in considering those dimensions that are more important than others\n",
    "\n",
    "3. Euclidean Distance is proved to be less reliable in in case of high-dimensional spaces due to the increased sensitivity. While, Manhattan Distance is able to handle high-dimensional data better than euclidean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dafdf-1d64-43ea-9d4c-8aa9326f3b50",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ab59677-655d-47d5-8277-53d2f245bd66",
   "metadata": {},
   "source": [
    "There are few techniques for finding the optimal value of K.\n",
    "\n",
    "1. Cross Validation with GridSearchCV\n",
    "\n",
    "2. Leave One Out Cross Validatino\n",
    "\n",
    "3. Elbow method (In this method, we plot model performance against different values of K and then look for the \"elbow\" point where the performance stabilizes.)\n",
    "\n",
    "In simple words when we look at the plot, we observe that there is a point from where the graph abruptly changes creating a knee shape. At the point, we check the corresponding value of K and that value of K is the optimal value that we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd65c8-0a7f-456f-b50d-f7c29652fac7",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bb9899b-a027-4abe-83c4-3eea2ab8eb57",
   "metadata": {},
   "source": [
    "Euclidean Distance: It represents the shortest straight line distance between two points in Euclidean space. This type of metric is highly sensitive to outliers due to squared differences.\n",
    "\n",
    "Performance Impact: We should consider this metric when we have continuous data and scenarios where the physical distance or direct path is meaningful. However, this metric might not perform very good with high-dimensional spaces due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "raw",
   "id": "92b45ab6-96f0-457e-beec-77621bc87ee5",
   "metadata": {},
   "source": [
    "Manhattan Distance: It denotes the sum of absolute differences along each dimension. This is less sensitive to outliers due to the use of absolute differences.\n",
    "\n",
    "Performance Impact: We might consider this metric when we have categorical data or scenarios where movement is restricted to grid-like paths. It has the abiity to deal with high-dimensional data far better as compared to Euclidean distance thus making it more suitable for high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2675ff-ee42-4b5c-81b2-3dde91685c4c",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d6fbdc7-8194-4ac1-b3b1-b4205b3b77cc",
   "metadata": {},
   "source": [
    "Some of the common hyperparameters in KNeighborsClassifier are as follows:\n",
    "    \n",
    "    n_neighbors,\n",
    "    weights,\n",
    "    algorithm,\n",
    "    leaf_size,\n",
    "    p,\n",
    "    metric,\n",
    "    metric_params,\n",
    "    n_jobs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a3275d8-4573-424f-a868-5a640cd6939b",
   "metadata": {},
   "source": [
    "Some of the common hyperparameters in KNeighborsRegressor are as follows:\n",
    "\n",
    "    n_neighbors,\n",
    "    weights,\n",
    "    algorithm,\n",
    "    leaf_size,\n",
    "    p,\n",
    "    metric,\n",
    "    metric_params,\n",
    "    n_jobs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d00bfa5-de37-4861-8ed3-0ba5bf2bf28f",
   "metadata": {},
   "source": [
    "These parameters plays a vital role in deciding the performance of KNN model. To find out the optimal value for each parameter, we simply use GridSearchCV that executes with all the possible input parameters and then give us a final combination of parameters along with their specific values.\n",
    "\n",
    "By doing this, we get the highest accuarcy from our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2dd8d-2b90-484c-93f5-f92175526dae",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8dc7eb1-aaf0-4b3c-99c3-09847ca69c29",
   "metadata": {},
   "source": [
    "SIZE OF DATASET:\n",
    "    \n",
    "    1. If the dataset is very small, it might cause underfitting.\n",
    "    2. If the datset is too large, it will lead to overfitting.\n",
    "    \n",
    "CROSS VALIDATION:\n",
    "\n",
    "We can use techniques like k-fold cross-validation to evaluate the models performance across different subsets of training data. This will help in finding out how well the model generalizes the data and whether there is a need to increase training set size or not.\n",
    "\n",
    "OUTLIERS:\n",
    "\n",
    "Outliers can negatively impact model performance. Hence, it is very important to ensure detection and removal of outliers from training set so that model learns meaningful patterns.\n",
    "\n",
    "Therefore, optimizing the training dataset is very crucial because we have to achieve a balance between model complexity, computational resources, and generalization performance. So, performing experiments with different techniques and evaluating their impact is vital to find out optimal training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b5416-d57a-41d5-b5d4-2213daa633a2",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cac0aae4-769e-4c7d-a847-d546e3b39304",
   "metadata": {},
   "source": [
    "KNN is a very powerful technique for detecting accuracy but at the same time, there are few drawbacks as well listed below:\n",
    "\n",
    "1. KNN can be computationally intensive whenever we are dealing with large training sets or high-dimensional data.\n",
    "\n",
    "2. KNN is sensitive to outliers as it considers all neighbors equally, including outliers that might add unnecessary noise to the data.\n",
    "\n",
    "3. Whenever we have high-dimensional spaces, the concept of nearest neighbors becomes less meaningful due to the increased sparsity of the data.\n",
    "\n",
    "To over these issue, we might consider following remedies:\n",
    "\n",
    "1. We can simply implement dimensionality reduction technique like PCA which will not only reduce the complexity but also help in achieving high accuracy.\n",
    "\n",
    "2. We should preprocess the data to detect and remove outliers before feeding it into the KNN model.\n",
    "\n",
    "3. Also, we can use oversampling technique like SMOTE to balance the class distribution in the training data. This can be done if we have imbalance dataset because due to imbalance dataset, we might get biasness during the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03938b8-7e30-4285-b97f-d6a6db9cf059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
