{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34904460-7a7a-41ea-ae2b-56e82e9d6800",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92a41bdf-f9c9-4005-83db-a24b2da07ca2",
   "metadata": {},
   "source": [
    "R-Squared (or coefficient of determination) is a statistical measure that explains proportion of variance in dependent variable that is predictable from independent variables. In simple words, R-squared shows how perfectly our data fits the regression model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1862a2bd-f166-4719-9a7a-e3e39239e1b6",
   "metadata": {},
   "source": [
    "FORMULA:\n",
    "    \n",
    "    R_Squared = 1 - SS(residual)/SS(total)\n",
    "    \n",
    "    where: SS(residual) = Residual sum of squares\n",
    "           SS(total) = Total sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "774c2e36-990b-4917-9949-da2760706df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see an example:\n",
    "\n",
    "observed_data = [3,5,7,9]\n",
    "predicted_data = [2.8,4.9,7.1,9.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7467bead-a7ed-4910-a9dc-fe5c017b8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc693b7-e07e-4e8b-af1c-06440edb7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mean = np.mean(observed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed47bd70-a17c-4ff4-b4cb-f6499c49c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_total = []\n",
    "for i in observed_data:\n",
    "    Total_SS = (i - obs_mean)**2\n",
    "    SS_total.append(Total_SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5754d77b-87a5-4b2b-8e28-d33881a6b57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.0, 1.0, 1.0, 9.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "266d312a-0552-4917-8f86-c00e8ffc7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_residual = []\n",
    "for i in range(len(observed_data)):\n",
    "    K = (observed_data[i] - predicted_data[i])**2\n",
    "    SS_residual.append(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0cf5bda-aacf-4e86-ad9a-06abd7805b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04000000000000007,\n",
       " 0.009999999999999929,\n",
       " 0.009999999999999929,\n",
       " 0.039999999999999716]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SS_residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c314e535-6bb8-4242-88e3-79aee57dc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(SS_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85280381-ef8d-4d56-911d-cd131646adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.sum(SS_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e48809b3-a4e5-4fdd-9574-3f83e7d34c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating R_Square\n",
    "\n",
    "R_square = 1 - (b/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60139577-5896-4b12-ad5a-d2414393f742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_square"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86132c0b-80d9-4d63-9a02-79030cdf379f",
   "metadata": {},
   "source": [
    "The R_sqaure came out to be 99.5% which means that the model has perfectly fitted the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34495c33-a03e-4baf-89a8-ef638c9fda3a",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "raw",
   "id": "02975e8e-98a3-4578-8c1f-ceb5bd3387bc",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that accounts for the number of independent features in a regression model. It adjusts the R-squared value on the basis of number of independent featueres and sample size hence giving more accurate result in terms of goodness of fit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a92862d-1099-42d2-9b46-71b1bfae8575",
   "metadata": {},
   "source": [
    "FORMULA:\n",
    "    \n",
    "    Adjusted R_square = 1 - [(1 - R_square)(n - 1)/(n - k - 1)]\n",
    "    \n",
    "    where: R_square is the normal r_square value\n",
    "           n = sample size\n",
    "           k = number of independent features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba013d-a252-4625-ae01-c788869f1de7",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "035103f9-0a03-484c-99f4-3c4f2adc1df7",
   "metadata": {},
   "source": [
    "It is recommended to go with adjusted R_square when we have multiple regression.\n",
    "\n",
    "If we talk about normal R_square, it increases the accuracy with increase in the number of features(even if they are useless) due to which we dont get precised accuracy. Thats why we use Adjusted R_square that penalizes the addition of unnecessary features and helps in getting accurate result.\n",
    "\n",
    "Secondly, when we are dealing with complex models with lots of features, it is recommended to use adjusted R_square as it only selects the useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9994b3f-343e-4d72-be0f-dc8c728557fb",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "330a45b7-f403-42e0-a8e5-b42f2f925830",
   "metadata": {},
   "source": [
    "These are some of the common metrices used to calculate the performance of regression models.\n",
    "\n",
    "RMSE is Root Mean Square Error (It is calculated by taking square root of the average of the squared differences between predicted and actual values)\n",
    "\n",
    "MSE is Mean Square Error (It is calculated by averaging the squared differences between predicted and actual values)\n",
    "\n",
    "MAE is Mean Absolute Error (It is calculated by averaging the absolute differences between predicted and actual values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b9465-2d41-4522-a0c9-e0958c005c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate these metrices by importing the below library:\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd78056-ab02-4a89-a182-43c6ebd65dca",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fc0f79e-bae6-4417-b9c9-dfe5a7bada60",
   "metadata": {},
   "source": [
    "RMSE\n",
    "    \n",
    "Advantages:\n",
    "    In situations, where outliers are important and need to be penalized, RMSE provides better result.\n",
    "    RMSE allots more weight to large errors due to the squaring operation, making it sensitive to outliers.\n",
    "    \n",
    "Disadvantages:\n",
    "    RMSEs sensitivity to large errors may not always show us the true performance of the model\n",
    "    \n",
    "MSE\n",
    "\n",
    "Advantages:\n",
    "    Just like RMSE, MSE also penalizes larger errors which makes it more useful for emphasizing the impact of large deviations in model predictions.\n",
    "\n",
    "Disadvantages:\n",
    "    MSE is also sensitive to outliers and can give undue importance to extreme values affecting the overall performance of model.\n",
    "    \n",
    "MAE\n",
    "\n",
    "Advantages:\n",
    "   MAE is less sensitive to outliers as compared to RMSE and MSE which makes it more robust against extreme values in the data.\n",
    "   It is very straightforward to interpret because it only involves performing average magnitude of errors without squaring them.\n",
    "   \n",
    "Disadvantages:\n",
    "   MAE can be less informative about impact of large errors since it treats all errors equally irrespective of their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320cd2e-8fe5-438f-bb1e-d6e7a19c6097",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f65dd113-506a-486a-b201-78d8c987dd3c",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique which mainly focuses on reduction of overfitting. It is also known as L1 regularization.\n",
    "\n",
    "Ridge regression focuses on feature selection. Also known as L2 regularization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4ef1c0b-bef4-4916-b976-832d04d98900",
   "metadata": {},
   "source": [
    "Lasso regression tries to reduce the coefficients of less important features to zero which leads to effectively performing of feature selection. It also produces sparse solutions by eliminating irrelevant features and generating more interpretable model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a69815ef-35eb-4f62-80a8-9f237bf764fb",
   "metadata": {},
   "source": [
    "When to use LASSO:\n",
    "\n",
    "We should consider using Lasso when we observe many features are irrelevant or redundant in our dataset and we want to obtain a sparse model. It is also suitable when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18c60c-d783-4258-90f3-d0539d384dcb",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "76a89df5-06eb-4c72-8f36-e320c1cff276",
   "metadata": {},
   "source": [
    "Regularized linear models such as Lasso and Ridge help us to prevent overfitting by adding a penalty term to the cost function which instructs the model not to fit noise in the training data too closely."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4ae75ca-2108-4276-b98b-4da0b868f560",
   "metadata": {},
   "source": [
    "Example: Lets say a dataset with 1000 samples and 20 features and our aim is to predict house prices based on these features. We will use a linear regression model with both Lasso and Ridge regularization and compare the results."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9dca60f7-508b-4c2b-8747-f901ad48db23",
   "metadata": {},
   "source": [
    "1. When we apply Lasso regularization, the model main function includes a penalty term that is the sum of the absolute values of the coefficients multiplied by a regularization parameter(alpha).\n",
    "This penalty will tends to set all the unnecessary feature coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "2. Similarly, when we apply Ridge regularization, the model main function includes a penalty term that is the sum of the squared coefficients multiplied by a regularization parameter(alpha). This penalty ignores large coefficients and helps to smoothen out the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc244f69-adea-4471-9096-321633a0be36",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11d95548-6dc0-49ab-a48e-1f45f39a2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_model_A = 10\n",
    "MAE_model_B = 8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0db55df7-099c-49a2-95ab-589fb4756b7b",
   "metadata": {},
   "source": [
    "RMSE gives more emphasis on larger errors due to the squaring hence making it more sensitive to outliers and large deviations.\n",
    "\n",
    "In this case, Model A has an RMSE of 10, which means that its predictions are offset by 10 units."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f897a4b1-aaa4-4dff-87cc-2010e38bf037",
   "metadata": {},
   "source": [
    "On the other hand, MAE distributes equal weight to all errors irrespective of their magnitude making it less sensitive to outliers.\n",
    "\n",
    "Here, model B has MAE of 8, which means that its predictions are offset by 8 units."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ef595fc-e0aa-4d4a-9521-e83c4d22b8f8",
   "metadata": {},
   "source": [
    "CONCLUSION:\n",
    "    If our main objective is to minimize the impact of outliers and large errors, we will go with MAE.\n",
    "    \n",
    "    But if the larger errors need to be penalized more heavily, then we should go with RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f389f8e-e755-4404-bb93-faf4b6635c9f",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae13c6a4-8c91-4a52-98e7-ac8a548159d8",
   "metadata": {},
   "source": [
    "As we know, Ridge regularization adds a penalty term to the cost function that is the squared sum of the coefficients multiplied by a regularization parameter.\n",
    "Here, Model A uses paramter as 0.1 whhich represents a moderate level of regularization and helps in reducing multicollinearity and thus stabilizing the model by reducing the coefficients to nearly zero.\n",
    "\n",
    "On the other hand, Lasso regularization also adds a penalty term to the cost function that is the sum of the absolute values of the coefficients multiplied by a regularization parameter.\n",
    "Here, Model B uses regularization parameter of 0.5 which results in generation of a sparse solutions by making some of the coefficients exactly to zero."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e11e476c-1396-4f04-9d5c-925c71add0be",
   "metadata": {},
   "source": [
    "If our objective is to prioritize model interpretability and feature selection, Model B might be preferred.\n",
    "\n",
    "If multicollinearity is a major concern and we want to reduce its impact, then Model A might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f30dff-c945-4922-ac9c-9df62a515dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
