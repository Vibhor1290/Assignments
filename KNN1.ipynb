{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd533db-b35d-48f1-b3eb-5751b8f3ca4e",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19f428a2-7863-4ec3-a7c0-4ca8a3980e1b",
   "metadata": {},
   "source": [
    "The K-nearest neighbors is a simple and non-parametric algorithm(lazy learning algorithm) used for classification and regression problem statements.\n",
    "\n",
    "In this method, we usually predict the category to which dataset belongs.\n",
    "\n",
    "Example: To predict the category of a datapoint, we simply select a specific number of neighbouring datapoints surrounding it. Lets say, we take 5 closest points from our new datapoint. Now, we will check the category of each data point.\n",
    "\n",
    "Out of those 5 datapoints, we will observe the category of each point and then take out the prevailing category(or the category with majority of occurrence).\n",
    "\n",
    "Let say, out of the 5 neighbors 3 neighbors belong to class A and rest 2 belong to class B. From this, we will conclude that the new datapoint will belong to the class A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a75532-8280-40ec-877e-c6395f340988",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30874096-0026-4432-a7c6-4c08716413ef",
   "metadata": {},
   "source": [
    "We can select an optimal value of K using following techniques:\n",
    "    \n",
    "    1. Cross Validation (we can perform cross validation using GridSearchCV)\n",
    "    \n",
    "    2. Leave One Out Cross Validation (special case of k-fold cross-validation where k is number of training instances. Each instance in the dataset is used as single test case while the remaining instances becomes the training set.\n",
    "    \n",
    "    3. Empirical testing: In this method, we simply use different random values of k and then try to find out the best value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f189c-03bd-4dba-aeba-53c365087d54",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60b15259-f52e-4496-a569-7c874836f2fd",
   "metadata": {},
   "source": [
    "The main difference is the situation/scenario when they are used.\n",
    "\n",
    "KNN Classifier is used for handling categorical problem while KNN Regressor is used for handling continuous problem.\n",
    "\n",
    "Example: KNN classifier is suitable for tasks where our aim is to categorize data into discrete classes like spam detection, image classification, or disease diagnosis.\n",
    "\n",
    "       : KNN regressor is suitable for predicting continuous values like house prices, stock prices, or temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d32f58-a8ce-4b1a-aa7a-e7077aa32f1a",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91dbe28b-fd6a-459f-8c4d-547c4bd62a78",
   "metadata": {},
   "source": [
    "1. Accuracy: Number of correct predictions/Total number of predictions\n",
    "\n",
    "2. Precision: True Positive/(True Positive + False Positive)\n",
    "\n",
    "3. Recall: True Positive/(True Positive + False Negative)\n",
    "\n",
    "4. F1_score: 2*Precision*Recall/(Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8eaf04-3280-41bc-9c24-326eb117fd11",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e7b8f31-4c01-439d-9261-d0c0e41d0c63",
   "metadata": {},
   "source": [
    "The Curse of Dimensionality in KNN refers to the challenges that we encounter when working with high-dimensional data. As we increase the number of features or dimensions, the volume of feature space grows exponentially, leading to sparsity in the data distribution."
   ]
  },
  {
   "cell_type": "raw",
   "id": "73656fab-2cb9-423e-9441-33b526a12e06",
   "metadata": {},
   "source": [
    "If we have a 1D or 2D space, neighbors are easy to identify but the number of points needed to maintain the same density grows.\n",
    "In case of 10D space, the number of data points needed to achieve a similar density grows exponentially which makes it difficult to find true nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefd8d1-49ca-4716-8781-b3c3d40ac46e",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea46a4d0-799d-4452-be67-8e845df9d2de",
   "metadata": {},
   "source": [
    "There are various techniques of hsndling missing data:\n",
    "    \n",
    "    1. Removing row/column having missing values (use this technique only if the number of missing values is small otherwise we might face data loss)\n",
    "    \n",
    "    2. Mean/Median/Mode Imputation: We can simply replace the missing values with mean, median or mode\n",
    "    \n",
    "    3. KNN Imputation: Use the KNN algorithm itself to impute missing values. This involves finding the k-nearest neighbors for each instance with a missing value and imputing the missing value based on the mean/median of the corresponding values of the neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d27038-987c-4d4a-95a7-3299db4d31c3",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5e608d3-271e-4838-be73-c57aa2a45bee",
   "metadata": {},
   "source": [
    "Type of Output:\n",
    "KNN Classifier: For finding discrete class labels.\n",
    "KNN Regressor: For finding continuous values.\n",
    "\n",
    "Decision Process:\n",
    "KNN Classifier: Uses majority voting among neighbors.\n",
    "KNN Regressor: Uses average/weighted average of neighbors values.\n",
    "\n",
    "Performance Metrics:\n",
    "KNN Classifier: Evaluated using metrics like accuracy, precision, recall, F1-score, confusion matrix, etc.\n",
    "KNN Regressor: Evaluated using metrics like MAE, MSE, RMSE, and R2 score.\n",
    "\n",
    "Use Cases:\n",
    "KNN Classifier: Suitable for dealing problems where outcomes are categorical like spam email detection, weather forecasting, etc.\n",
    "KNN Regressor: Suitable for dealing problems where outcomes are numerical and continuous like price predicting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7564a6-435b-41b1-af1f-bd31325481a8",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4a6327c-b4dc-4471-8664-c0d0f39a147b",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "    \n",
    "    1. Easy to understand and implement.\n",
    "    \n",
    "    2. KNN is a non-parametric algorithm which means that it does not take any assumptions about the underlying data distribution.\n",
    "    \n",
    "    3. It can effectively capture complex patterns in data.\n",
    "    \n",
    "Disadvantages: \n",
    "\n",
    "    1. Becomes computationally expensive as size of dataset increases\n",
    "    \n",
    "    2. Requires high memory usage, especially for large datasets.\n",
    "    \n",
    "    3. Model performance is very sensitive to value of K. Too small K can cause overfitting, while too large K can cause underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54e937-742d-4716-aa22-bee15c7d0a8a",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a63910ab-2616-424c-99a8-c42e7dcd8b0f",
   "metadata": {},
   "source": [
    "The main difference is with the formaula that is used by both of them.\n",
    "\n",
    "Euclidean Distance: sqrt([x2-x1]**2 + [y2-y1]**2)\n",
    "\n",
    "Manhattan Distance: |x2- x1| + |y2 - y1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adc0b1-917f-428c-ad80-b41cf58d001d",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bedbef2e-4bc8-49ea-8f6b-8917e49dd66c",
   "metadata": {},
   "source": [
    "Feature scaling plays a vital role in K-nearest neighbors (KNN) algorithm because KNN solely depends upon distance metrics to determine the closeness of instances. If features are not scaled, all those feature with larger range can disproportionately affect the distance calculations, leading to biased results.\n",
    "\n",
    "1. Feature scaling ensures that all features contribute equally to the distance metric.\n",
    "\n",
    "2. Scaling results in better performance as KNN algorithm correctly identify nearest neighbors based on all features equally.\n",
    "\n",
    "3. Scaling ensures that the distance metric accurately reflects the relative differences among all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c69a3-f810-440b-b96a-869c260daff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
