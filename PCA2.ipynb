{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0dc5e0-fca7-464c-8b49-f6a9e694b3e5",
   "metadata": {},
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4713f0b9-b25a-4ec1-84a8-1073129606c7",
   "metadata": {},
   "source": [
    "In the context of PCA, projection refers to the process of transforming the data into a new coordinate system defined by the principal components."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0550e421-751c-4d44-97be-a4fc8f2329dc",
   "metadata": {},
   "source": [
    "Step1: In the initial step, we center the data by subtracting the mean of each feature from the dataset. Doing this ensures that data has a mean of zero, which is essential for PCA to work correctly.\n",
    "\n",
    "Step2: The next step is to compute the covariance matrix which simply captures the variance and relationships between different features in the dataset.\n",
    "\n",
    "Step3: After that, covariance matrix is broken down into its eigenvalues and eigenvectors. The eigenvectors represent the directions of maximum variance and eigenvalues represent the magnitude of the variance.\n",
    "\n",
    "Step3: A subset of principal components is choosen on the bais of eigenvalues. Hence, the components with largest eigenvalues are chosen as they capture the maximum variance in the data.\n",
    "\n",
    "Step5: Finally, the original data is projected onto this selected principal components. This is basically done by multiplying original data matrix and selected eigenvectors matrix. This transformation results in a new dataset with reduced dimensions, where each dimension corresponds to one of the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81de68-1778-4f6f-bfe1-e0ab347acd38",
   "metadata": {},
   "source": [
    "How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c3b44a2-e28b-489f-a68d-519a18106cf6",
   "metadata": {},
   "source": [
    "The main aim of PCA is to reduce the dimensionality of a dataset while retaining as much information as possible. This is done by identifying the principal components along which the variance of the data is maximum."
   ]
  },
  {
   "cell_type": "raw",
   "id": "282b0d92-d907-497b-a2b5-f9eb2363db7d",
   "metadata": {},
   "source": [
    "As we know that first principal component captures the most variance, the second captures the second most, and so on. Hence, by selecting the top number of principal components, we can reduce the data dimensionality to k dimensions while retaining most of the original variance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb1ea5b8-17e1-4df0-add3-3d8b7acb6847",
   "metadata": {},
   "source": [
    "The optimization problem in PCA aims to identify that principal component which maximizes the variance of the data when projected onto these directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439274c-cc6b-4a99-b25a-da99d5a4e3b2",
   "metadata": {},
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "805a3f4f-55a4-468a-9497-c21175187e23",
   "metadata": {},
   "source": [
    "Prior to evaluating the covariance matrix, the dataset is typically centered by subtracting the mean of each feature which make sure that data has a mean of zero and we can easily interpret covariance matrix.\n",
    "\n",
    "After data centering, we calculate the Co-variance matrix by using below formula:\n",
    "\n",
    "S = 1/(n-1)*(X.t*X)\n",
    "\n",
    "where n = number of observations,\n",
    "      X = centered dataset/matrix\n",
    "      \n",
    "      The Co-Variance matrix is a PxP matrix where each element represents Co-Variance between the features. Here, P = number of features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcdb46c5-4347-4a26-8bc5-2f57546b468e",
   "metadata": {},
   "source": [
    "There is a vital role of covariance matrix in PCA. It captures the variance of each feature and the correlations between pairs of features. High absolute values of covariance indicate strong linear relationships between features and vice-versa."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6dd2b44-7240-4763-9fbe-7751dc196a44",
   "metadata": {},
   "source": [
    "The eigenvectors define the directions in which the data varies the most while eigenvalues represents the amount of variance captured by each principal component.\n",
    "\n",
    "After we achieve the eigenvalues, they are sorted in descending order along with the corresponding eigenvectors similarly. Later on, principal components are then selected on the basis of largest eigenvalues, which capture the most variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59361e4a-dc05-46e0-b758-b52a967f8bbd",
   "metadata": {},
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "110e8b37-c43e-41c4-ad05-6952823e0da5",
   "metadata": {},
   "source": [
    "1. If a less number of principal components are chosen, it might lead to loss of a the data variance which can further result in loss of important information and cause poor representation of the original dataset.\n",
    "On the other hand, considering too many components might retain unnecessary noise and redundant information causing accuracy reduction.\n",
    "\n",
    "2. Using fewer components can reduce computational time and memory usage, making the processing of large datasets efficient. While, using more components result in higher computational costs, both in terms of time and memory.\n",
    "\n",
    "3. Retaining too many components can lead to overfitting, where the model captures unnecessary noise. Similarly, fewer components may lead to underfitting, where the model is too simple to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de51f41-0746-42f9-87e7-541293729668",
   "metadata": {},
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e35c534b-b554-4bb8-8b76-1bf45e66f22d",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used in feature selection to reduce the dimensionality of a dataset while retaining the most important information.\n",
    "\n",
    "1. We implement PCA in the dataset to calculate the number of principal component. These components are linear combinations of original features and represent the directions in which the data varies the most.\n",
    "\n",
    "2. Also, we use explained variance ratio which capture a significant portion of variance are those components with higher variance are selected.\n",
    "\n",
    "3. Furthermore, we can also perform transformation of original data into new subspace having specific number of selected principal components. This helps in achieving a reduced set of features which captures most of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31033e6c-be55-4fe0-b7d0-ba0a19752983",
   "metadata": {},
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b67e5bc-585c-44e5-9865-6a33af51625c",
   "metadata": {},
   "source": [
    "1. PCA is used to reduce high-dimensional data into 2D or 3D spaces thus making it possible to visualize complex datasets more clearly.\n",
    "\n",
    "2. By reducing the number of features, we can redcue the likelyhood of curse of dimensionality further improving the performance of ML algorithms and reduced computational costs.\n",
    "\n",
    "3. By focusing on only highest variance components, PCA helps in removing components that are likely to be noisy.\n",
    "\n",
    "4. PCA helps in compressing data by reducing number of variables, which is essentially useful for storage and transmission purpose of large datasets."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa1034da-2537-4463-8bf2-78344570ad2f",
   "metadata": {},
   "source": [
    "Image Compression: PCA is used in image compression techniques to reduce number of pixels while retaining essential image features.\n",
    "\n",
    "Fraud Detection: PCA is used in fraud detection to identify any unusual behavior in transaction data, which could indicate fraudulent activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3589bc5-6778-4dd1-90bb-269ad4084c05",
   "metadata": {},
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ff5f039-6029-43b7-8d36-4037b075f03d",
   "metadata": {},
   "source": [
    "Variance is a statistical measure which shows that how much a data points is deviated from its mean.\n",
    "\n",
    "While, spread refers to the distribution of data points in a dataset by describing how much the data points are dispersed or scattered."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b80ed1e4-91c6-4c32-86c1-b72fe9a95fbf",
   "metadata": {},
   "source": [
    "In PCA, variance is very important because the principal components are the directions where the data varies the most. The goal of PCA is to find these principal components and project the data onto them.\n",
    "\n",
    "The spread of data is directly related to its variance. A larger spread means greater variability among the data points resulting in higher variance and vice-versa."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ee8ae21-44a5-43d7-b003-2c3aa3d47cf7",
   "metadata": {},
   "source": [
    "The first principal component captures the maximum variance in the data followed by second principal component and then so on.\n",
    "\n",
    "The variance captured by each principal component is calculated using its corresponding eigenvalue. A larger eigenvalues indicate those components which captures more variance (or more spread) in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3547d1e-9a24-49f2-b257-42702fd647a1",
   "metadata": {},
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9941b1f3-216b-4bf0-ae22-1ac36c451dd6",
   "metadata": {},
   "source": [
    "In short, below are the steps of how we use spread and variance of data to identify principal components:\n",
    "\n",
    "1. Centering the data to have a mean of zero.\n",
    "\n",
    "2. Calculating covariance matrix to understand the spread and relationships between features.\n",
    "\n",
    "3. Finding the eigenvalues and eigenvectors of the covariance matrix to determine the directions of maximum variance.\n",
    "\n",
    "4. Sorting these eigenvalues and eigenvectors to select principal components with maximum variance.\n",
    "\n",
    "5. Transforming the original data into the new space defined by the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a4647-4249-43b9-af01-e447e1908be0",
   "metadata": {},
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20a376ad-eb09-4743-8f1b-a0e6255d8ae8",
   "metadata": {},
   "source": [
    "PCA works by identifying the principal components in the data that capture the maximum variance. If some of the dimensions have high variance, they will contribute more to the principal components and vice-versa.\n",
    "\n",
    "ALso, the eigenvalues associated with each principal component indicate the variance explained by that component. High eigenvalues denotes the dimensions with high variance while low eigenvalues represents dimensions with lower variance.\n",
    "\n",
    "After we have identified and retained the most essential principal components, PCA transforms this data into a lower-dimensional space(either 2D or 3D). This transformation reduces the dimensionality of data and at the same time captures as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5957e6-8b11-4ea4-a2a7-1392327517ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
