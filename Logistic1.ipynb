{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117d9bd8-7e8a-4fad-ad39-20db39099b55",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e30778-8aed-42c1-a312-ccc188008250",
   "metadata": {},
   "source": [
    "Linear regression is used for prediction of a continuous dependent feature based on one or more independent features. While Logistic regression predicts a categorical dependent feature based on one or more independent features.\n",
    "\n",
    "The objective of linear regression is to find a best-fit line between the dependent and independent features. On the other hand, logistic regression evaluates the probability that a specific input belongs to a certain class/category."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f346a32-3ff0-4ea2-af69-e6763a2ee0a9",
   "metadata": {},
   "source": [
    "We will consider logistic regression under following conditions:\n",
    "\n",
    "1. The dependent feature is categorical(oe simply binary).\n",
    "\n",
    "2. Suppose we have to estimate the probability of a person being a fraud, we use logistic regression along with sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2991b7-8e71-450f-b6df-17a901ff81b4",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34346188-8798-4081-9a0c-1ac7c1d6a2a0",
   "metadata": {},
   "source": [
    "In logistic regression, we use log loss(binary cross-entropy) loss function. The output of this function lies between 0 and 1.\n",
    "\n",
    "To optimize the cost funtion, we make few modifications with theta.\n",
    "\n",
    "Initially, we start with placing theta equal to either 0 or with a very small value.\n",
    "\n",
    "Then, we evaluate the gradient of cost function wrt each parameter theta. Then, the parameters are updated using gradient.\n",
    "\n",
    "This same process is repeated again and again until the cost function comes to a minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d06a05-7b4b-4f85-9369-f1fd481bdab4",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b07ed119-f255-4021-a954-6bb85ec6df92",
   "metadata": {},
   "source": [
    "There are 2 most common regularizations applied in logistic regression, one is L1 regularization and the other is L2 regularization.\n",
    "\n",
    "Both these regularization adds a penalty terms to the coefficients of features.\n",
    "\n",
    "By adding a penalty to large coefficients, regularization tells the model to keep weights small hence reducing the model complexity. This further avoid the model to fit noise in the training data.\n",
    "\n",
    "A simple model with smaller weights is less likely to capture noise and more likely to give better accuracy.\n",
    "\n",
    "L1 regularization can eliminate some features by setting their coefficients to zero thus reducing the model complexity as well as overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805069b3-e843-48c7-9de1-cbc337b984a6",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a30a82d5-85b3-43bb-b2f8-9b8740fdb94d",
   "metadata": {},
   "source": [
    "ROC curve generally shows a plot between True Positive Rate against False Positive Rate at different threshold values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8127fc8-a9bc-4573-8bcd-fefcc1860178",
   "metadata": {},
   "source": [
    "True Positive rate is ratio of true positive and true positive + false negative.\n",
    "\n",
    "     TPR = TP/(TP + FN)\n",
    "    \n",
    "False Positive Rate is the ratio of false positive and false positive + true negative.\n",
    "\n",
    "    FPR = FP/(FP + TN)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20ceb37f-aa34-4f84-a995-f40f0ae9ec91",
   "metadata": {},
   "source": [
    "On x-axis, we plot the FPR and on y-axis, we plot TPR. Each point on the ROC curve represents a different threshold. Finally, the area under ROC curve gives us the summarized performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73236951-299d-4120-a453-ef7ebd55ba0e",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f91ceda-2f99-415d-94c7-772cbad006d5",
   "metadata": {},
   "source": [
    "1. Filter Method:\n",
    "    \n",
    "    In this method, relative importance is calculated between independent and dependent features. This is evaluated using techniques like ANOVA, Chi-Square, or we wcan look at the intrinsic properties of the data.\n",
    "    \n",
    "2. Wrapper Method:\n",
    "\n",
    "    In this method, we train the model multiple times with different subset of features and then select the best performing subset. This technique can be performed in 2 ways: Forward or Backwerd. In forward, we start with one feature and then add the features subsequently. While in backward, we start with all the features at once and then reomve the features one by one.\n",
    "    At each stage of either forward or backward approach, we carefully observe that which feature is resulting in either an increase or decrease in the performance.\n",
    "    \n",
    "3. Embedded Method:\n",
    "\n",
    "    In this method, we use some of the common techniques like L1 regularization, random forest, gradient boosting, etc.\n",
    "    \n",
    "4. Dimensionality Reduction\n",
    "\n",
    "    Under this method, we mostly use PCA(Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b328d1-0e7a-4518-a496-fc7e63f019e2",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08f4a492-ca23-4da5-b885-365973a5ad8e",
   "metadata": {},
   "source": [
    "Imbalance dataset is a dataset in which the ratio between the output feature categories is abrupt. Example: In a dataset, we have a binary output with True and False. Now, if we count the frequency of the output, we get that True is coming 80% and False is coming 20%"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bddf433-473a-42d3-ba6a-3335160b57ab",
   "metadata": {},
   "source": [
    "In simple terms, the ratio of the output is 8:2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "701a13f0-9be0-4918-b404-add875070e35",
   "metadata": {},
   "source": [
    "Now, to handle this type of problem, we use 2 techniques: Upsampling and Downsampling.\n",
    "\n",
    "In upsampling, we increase the lower ratio value and make it equal to the higher one.\n",
    "\n",
    "On other hand, in downsampling we reduce the higher ratio value and make it equal to the lower one.\n",
    "\n",
    "Example: We have the ratio as 8:2. Either we will make it 8:8 or 2:2."
   ]
  },
  {
   "cell_type": "raw",
   "id": "09d863b1-ce06-4dd9-8015-36ba389e8984",
   "metadata": {},
   "source": [
    "There is one more technique known as SMOTE (Synthetic Minority OverSampling Technique)\n",
    "\n",
    "In this technique, we usually perform upsampling. Here, we first join all the closest data points together in a line and then we use interpolation method to add new datapoints between these existing points within that line. By doing this, more data points gets added to the dataset and the imbalance problem is avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ccf56-a1be-447b-9bd0-9181704d31df",
   "metadata": {},
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "192191a8-d139-4bcc-9f7b-2ec867f3950a",
   "metadata": {},
   "source": [
    "There are some challenges that we might encounter while implementing Logistic Regression:\n",
    "    \n",
    "1. Multicollinearity\n",
    "\n",
    "To avoid this problem, we simply use L1 or L2 regularization that allots certain weightage to the features based on their relevance.\n",
    "\n",
    "Alos, we can use PCA(Principal Component Analysis) that creates a best fit line having characteristics of important features.\n",
    "\n",
    "2. Overfitting\n",
    "\n",
    "This is a very common issue that we face in nearly every model.\n",
    "\n",
    "To avoid this issue, we use L1/L2 regularization, Cross-Validation Techniques, etc.\n",
    "\n",
    "3. Underfitting\n",
    "\n",
    "To reduce this effect, we can simply add more features to the data.\n",
    "\n",
    "4. Imbalanced Dataset\n",
    "\n",
    "To solve this, we use upsampling, downsampling, SMOTE, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07d1f8-3bce-4833-8a37-3f21f3242053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
