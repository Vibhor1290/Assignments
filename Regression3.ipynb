{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679fca8f-c01a-450b-8045-faf8785b51e3",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57dcddc6-cfb1-4e1c-b6f5-ab7f02f2920e",
   "metadata": {},
   "source": [
    "Ridge Regression is used to analyze the relation between dependent and independent variables. It is somewhat similar to Ordinary Least Squares regression but the only difference is that it includes a regularization term called ridge or L2 penalty which prevents overfitting and reduces the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a44a79c8-cfff-46aa-b3fa-ea957cc63e1a",
   "metadata": {},
   "source": [
    "1. Ridge Regression usually provides more stable solutions as compared to OLS especially when we have a small dataset or when there are number of independent features relative to the number of observations.\n",
    "\n",
    "2. OLS reults in achieving low bias but high variance which means that it may increase chances of overfitting. Whereas, Ridge Regression introduces bias to reduce variance, leading to better generalization performance on unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2094c34-eb2c-45ae-b5d4-295727e2f23b",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7948793-8273-48ab-8184-f6c461ef0543",
   "metadata": {},
   "source": [
    "Assumptions: \n",
    "    1. Ridge Regression assumes a linear relationship between independent and dependent variable.\n",
    "    \n",
    "    2. The features being used in Ridge Regression are independent of each other.\n",
    "    \n",
    "    3. It assumes normality i.e the differences between observed and predicted values follows normal distribution.\n",
    "    \n",
    "    4. There is no perfect multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f9780-a696-418b-89ee-4c0a3dfb8686",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "488efc76-354b-4a49-a4a4-9c0d263765a5",
   "metadata": {},
   "source": [
    "To decide the value of lambda for ridge regression, we can use following techniques:\n",
    "\n",
    "1. Grid Search (In this, we have an estimator and param_grid. Using these 2 parameters, we find out the best value of lambda)\n",
    "\n",
    "2. Cross Validation (This is a very common technique for hyperparameter-tuning.\n",
    "\n",
    "Example: In K-fold CV, the data is split into k subsets and the model is trained and validated k times, each time using a different subset as the validation set. After this, we get k number of outputs and we simply calculate the mean from those values to get a single value.\n",
    "\n",
    "3. Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294717a8-16c9-4b3a-b470-211cb0e4cd50",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d23dc43-d5d6-4b9c-b231-c74fb40f5e3f",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for feature selection. However, it is not as easy as Lasso Regression in which we explicitly set some of the coefficients to zero."
   ]
  },
  {
   "cell_type": "raw",
   "id": "59fdb2be-1662-4eb4-99f9-ac94797ab843",
   "metadata": {},
   "source": [
    "This can be done by following methods:\n",
    "    \n",
    "    Ridge Regression pushes the coefficients towards zero(not exactly to zero) unless lambda is extremely large. As the value of lambda increases, magnitude of coefficients decreases and some of the coefficients become very close to zero.\n",
    "    \n",
    "    We can observe the magnitude of the coefficients in Ridge Regression model and see which features offers larger coefficients and which one offers smaller coefficients. The one with larger are catered as important while the one will lesser are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503aa903-440c-4548-a2ef-1c743a937c06",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df998864-100e-44ed-96ec-cb5f1a603e4a",
   "metadata": {},
   "source": [
    "1. The presence of multicollinearity in Ridge Regression model is handled by adding a regularization term to the cost function which reduces the variance of the coefficients which further, helps to stabilize the coefficient estimates and make them less sensitive to multicollinearity.\n",
    "\n",
    "2. Ridge Regression introduces bias by reducing the coefficients towards zero hence reducing the model's variance. In the context of multicollinearity, this bias-variance tradeoff is very important because it prevents the model from fitting the data too closely to the noise.\n",
    "\n",
    "3. In the presence of multicollinearity, Ridge Regression can still provide meaningful insights from the data. This is achieved by reducing the coefficients of correlated features and using this method, Ridge Regression indirectly highlights which all features are more impactful in predicting the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89d75e-d45c-403a-affc-d881a665ef28",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "959fe0e3-e664-40ed-b737-21803f4ebeb5",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both categorical and continuous independent variables.\n",
    "\n",
    "However, we must keep few things in mind like:\n",
    "\n",
    "1. Ridge Regression model requires numerical input. So, we will have to convert categorical data into numerical data. This can be done using encoding techniques like one-hot encoder, label encoder or target guided ordinal encoder.\n",
    "\n",
    "2. Ridge Regression is sensitive to the scale of features, so it is very important to scale down the continuous features to a certain range. This scaling can be done using StandardScaler method.\n",
    "\n",
    "3. When we have a huge number of unique values in a categorical feature, it might introduce more parameters to the model. Due to this, we might encounter overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04fcc0-0a4b-4c48-9ffd-71cc9dc5ce35",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5b6ecd7-13d9-4737-af9f-d5c736b64da2",
   "metadata": {},
   "source": [
    "Magnitude:\n",
    "    \n",
    "    A feature having higher magnitude means that particular input feature is very strongly dependent for independent feature.\n",
    "    \n",
    "    A feature having lower magnitude means that particular input feature is weakly dependent for independent feature.\n",
    "    \n",
    "Direction:\n",
    "    \n",
    "    A postivie direction shows direct relation b/w the input and output feature.\n",
    "    \n",
    "    A negative direction shows inverse relation b/w the input and output feature.\n",
    "    \n",
    "    \n",
    "Regularization:\n",
    "\n",
    "   This regularization effect means that if a features's coefficient is not exactly equal to zero, it means that feature is relatively small and but might contribute atleast something to the prediction as compared to other feature's coefficients with larger magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc63170-b4c8-4dc4-a27d-6cc67e757a29",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "473bb52c-1a8c-49bf-8b54-a62bb89ea33a",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis, especially when handling regression tasks in which dependent feature is highly impacted by a combination of continuous and categorical independent features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b288f8d-89f9-47f9-a28b-981bc7f7f0b8",
   "metadata": {},
   "source": [
    "1. Time-series data often includes patterns like trends, seasonality, and autocorrelation. Hence, Ridge Regression can be used to model these patterns by including relevant time-related features as independent features.\n",
    "\n",
    "2. Ridge Regression's regularization improves the stability of the model, usually when handling multicollinearity among the features or when the number of features is huge as compared to number of observations in the time series.\n",
    "\n",
    "3. We can also use Cross-validation technique to perform hyper-parameter tuning for lambda in Ridge Regression in time-series data analysis. This is done by dividing the time series data into training and validation sets and then evaluating different values of lambda and choosing the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2118d9-5c2b-4dea-bd6b-226d4d619ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
