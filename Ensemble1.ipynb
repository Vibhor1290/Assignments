{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d9e48d-8667-4c19-aa70-7cf9bcf4323c",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ace1bdf-0387-43b7-864e-706502a9a094",
   "metadata": {},
   "source": [
    "Ensemble techniques is a methodology that ombine multiple models to improve the overall performance, accuracy, and robustness of predictions compared to individual models. The main idea behind ensemble methods is to aggregate the performance of all the individual model and obtain a single accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdb6b2-8faf-4b41-a28c-87c8d134dd39",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fc9e14a-f150-4a35-8af0-8ea1841c84b5",
   "metadata": {},
   "source": [
    "The main reason behind using Ensemble techniques is:\n",
    "    \n",
    "    1. To get overall higher accuracy\n",
    "    \n",
    "    2. To reduce overfitting\n",
    "    \n",
    "    3. Obtain a balanced trade-off between bias and variance\n",
    "    \n",
    "    4. Enhance the robustness of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee37471-9f19-413a-ba38-7546c20785c1",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25dc696d-efee-4924-aac2-a88408d6a29b",
   "metadata": {},
   "source": [
    "Bagging is an ensemble technique which works as mentioned below:\n",
    "    \n",
    "Step1: We get a dataset and then we split the dataset into train and test part.\n",
    "\n",
    "Step2: Now, from the train dataset we create multiple samples and feed these samples to base learners. Here, base learners are nothing but the individual models that we use in bagging technique. (Before we feed these samples to base learners, we ensure row and feature sampling is done at random)\n",
    "\n",
    "NOTE: Here the models that we use are Decision Tree.\n",
    "\n",
    "Step3: After we provide the sample data to the base learners, all the models executes together and each model gives a certain output.\n",
    "\n",
    "Step4: Now, if the output is continuous we simple calculate the average of all outputs to get a single output. And if the output is categorical, we use something called voting classification, in which we observe the output from every model and choose the category which hold the majority in the output section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4022a5-ccd2-4bae-93d3-ca4176dec31f",
   "metadata": {},
   "source": [
    "NOTE: There is a scenario where all the train data does not get the chance to go into the train models. In simple words, there is some data which get left and does not get the opprotunity to go into the model for execution. This type of data is a waste for us. But we do not ignore this leftover data because if we do so, we might encounter data loss. So, to overcome this problem, we convert all the leftover data into validation data. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f12ffde6-99bb-451b-8361-dc766812cb3e",
   "metadata": {},
   "source": [
    "To convert all the leftover data into validation data, we set a parameter 'oob_score' equals to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0275b-f556-4f4e-aa9e-bdd3635d95ad",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58d11b0b-ede1-4731-ab07-ffe5710ae244",
   "metadata": {},
   "source": [
    "This technique is somewhat similar to bagging technique. In this boosting technique, we create train and test split from original data. Then, we divide this train data into multiple samples and prior to it, we ensure random row and feature sampling.\n",
    "\n",
    "After the sample data is created, we feed the sample data to weak leaners. Here, weak learners are nothing but a series of models that we saw in bagging technique.\n",
    "\n",
    "The only difference is that in weak learners, the models execute in a chronological manner which means the first model of weak learner execute initially.\n",
    "\n",
    "The first model executes, giving us some accuracy along with few errors. The errors of the first model are passed on to the second model.\n",
    "\n",
    "Again, the second model executes giving us some accuracy along with few errors. The errors of second model are then passed on to third model.\n",
    "\n",
    "This process keeps on repeating until we reach the last model. The last model is known as strong learner. This is because it has taken all the errors into consideration within this model chain and gives the highest accuracy after learning all the errors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "19c590ae-56db-470b-a4bc-aba62ba43474",
   "metadata": {},
   "source": [
    "Once this execution completes, we observe the accuracy of each model and then evaluate an overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53796ce2-985b-45f8-a21b-012650c0c748",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c231122b-bebd-4c3f-9c21-83fd128f633a",
   "metadata": {},
   "source": [
    "Yes, the ensemble technique is mostly better than executing individual models. The main reasin is that a single model will give us single accuracy. But in ensemble, we use a series of models and each model is capable to giving us an accuracy and thus we get multiple accuracy values in the output from where we can evaluate single accuracy using either statistical measure or voting calssification technique.\n",
    "\n",
    "Here, each model will give us an accuracy along with some errors and those errors will be passed on to the next model. By doing so, our model will be able to understand the pattern of error that might occur in real world data and provide precise accuracy."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7dcd93f9-6dc7-4108-aad3-1b0508d6311b",
   "metadata": {},
   "source": [
    "There are certain scenarios where ensemble technique might not work as expected like in terms of complexibility, data size and quality. Furthermore, deploying and maintaining the ensemble models can be little bit challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6a8a-273a-4499-937b-2ad51d2834fc",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fff7d8e-0b14-4b53-b491-61657e4dd64a",
   "metadata": {},
   "source": [
    "Step1: From the original sample data of size n, we draw a large number of bootstrap samples where each bootstrap sample is obtained by randomly selecting n observations with replacement from the original data.\n",
    "\n",
    "Step2: For each bootstrap sample, we calculate statistic like mean, median, or regression.\n",
    "\n",
    "Step3: We then sort the bootstrap sample statistics in ascending order.\n",
    "\n",
    "Example: For a 95% confidence interval, we identify the 2.5th percentile and the 97.5th percentile of the bootstrap distribution while these percentiles acts as lower and upper bounds of the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56276fbd-b44d-46e8-9072-95aede667327",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959fa8d6-865a-4e64-bea8-7886fca74db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Bootstrap Confidence Interval for the Mean: [91.74041249999999, 94.7450125]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "dataset = np.random.randint(34,153,2000,int)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 5000\n",
    "\n",
    "# Generate bootstrap samples and calculate bootstrap means\n",
    "bootstrap_means = np.array([np.mean(np.random.choice(dataset, size=len(dataset), replace=True)) for _ in range(B)])\n",
    "\n",
    "# Calculate the percentiles for the confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% Bootstrap Confidence Interval for the Mean: [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90caae-1bf7-4629-91bb-1ad1bcc6be31",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0542c40a-0c61-4d8d-9a8f-b9dd2777f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "mean_height = 15\n",
    "std_dev = 2\n",
    "CI = 0.95\n",
    "boot_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eaec422-94e9-4156-9cd3-9f5d087e9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Bootstrap Confidence Interval for the Mean Height: [14.03, 15.06]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)  # For data reproducibility\n",
    "testing_sample = np.random.normal(loc=mean_height, scale=std_dev, size=n)\n",
    "\n",
    "# Generating bootstrap samples and evaluating bootstrap means\n",
    "bootstrap_means = np.array([\n",
    "    np.mean(np.random.choice(testing_sample, size=n, replace=True))\n",
    "    for _ in range(boot_samples)\n",
    "])\n",
    "\n",
    "# Calculating 2.5th and 97.5th percentiles for 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% Bootstrap Confidence Interval for the Mean Height: [{lower_bound:.2f}, {upper_bound:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80095c-82da-4fb0-88af-ba44c79ad6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
